{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS189 Project T Final Notebook\n",
    "## Week 2: Testing/Training, Cross-Validation, and Bias-Variance\n",
    "## This is the solution notebook, so all code will be written out. In the student notebook, specific lines will be left blank for students to fill in\n",
    "    \n",
    "Topic: 9. Training/Testing, Cross-Validation, Bias-Variance\n",
    "\n",
    "By: Team Sean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "from helper_fns import *\n",
    "\n",
    "# data organization libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization libraries\n",
    "import plotly.express as px\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# modeling libraries\n",
    "import sklearn as sk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Setup\n",
    "[TODO paste the The Setup slide from the slide deck once that's finalized.\n",
    "\n",
    "Something like:\n",
    "\n",
    "YOU have been chosen to pioneer the development of a machine learning system that ranks startups for probability of success.  We give you profiles of previous startups, including their capital, resource costs, country, Public or Nonprofit status, the amount of success they achieved years later, and much more. Can you estimate the success rates for todayâ€™s slew of new (fictional) startups? The investment firms are waiting to hear about the insight you provide!\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "\n",
    "Prediction models that are used in the industry must be able to maintain accuracy on previously unseen data. At this point in your EECS education, you have only learned model assesment within the context of data the model has already seen. This creates a problem, because if we assess our model with the same data that was used to fit it, then we may overestimate how well our model does at prediction. After completing this assigment, students will know the methodology behind improving prediction models so they are ready for use in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Training Data\n",
    "\n",
    "The first thing to do is always to find out what you're working with. We load the data X and labels y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO load X and y\n",
    "X = np.array([[1, 2], [3, 4]]*10)\n",
    "y = np.array([2, 3]*10)\n",
    "\n",
    "# shuffle x and y?\n",
    "\n",
    "# Features present in the data in order. These column names will help you interpret trends you see in the data\n",
    "# This was originally nested alongside the numerical data, but OLS requires us to use a matrix instead of a dictionary\n",
    "# TODO populate this with the actual column names\n",
    "FEATURES = ['product name', 'price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO something like \"plot some of the features of startups with respect to their success rates. What do you notice about the correlations? Are there correlations?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feat_selector(features):\n",
    "    \"\"\"\n",
    "    Helper function to create a subset of the data that will only include certain features\n",
    "    A full list of features is defined in FEATURES\n",
    "    \"\"\"\n",
    "    for f in features:\n",
    "        assert f in FEATURES, \"'{}' is not defined in the varaible FEATURES!\".format(f)\n",
    "    def feat_selector(X):\n",
    "        indices = [FEATURES.index(f) for f in features]\n",
    "        return X[:, indices]\n",
    "    return feat_selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring Your Machine Learning Model\n",
    "\n",
    "In this project, we will focus on the general process behind training many machine learning models. To illustrate this, let's pull out the SVM models you have learned about this week!\n",
    "\n",
    "In particular, we will be using it as a classifier, so it will be called a Support Vector Classifier (SVC). Use [sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to implement the training step below, given hyperparameters C and gamma. You may find the examples in the documentation very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, C=1.0, gamma=0.2):\n",
    "    \"\"\"\n",
    "    X_train - Training data\n",
    "    y_train - Training labels\n",
    "    C - a hyperparameter for SVC\n",
    "    gamma - a hyperparameter for SVC\n",
    "    \n",
    "    Return reg, an instance of LinearRegression.fit() that represents the trained model\n",
    "    \"\"\"\n",
    "    ##### START #####\n",
    "    reg = SVC(C=C, gamma=gamma).fit(X_train, y_train)\n",
    "    ##### END #####\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO something about changing up the features passed in X_train based on the visualizations above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation\n",
    "\n",
    "How good are your hyperparameters? How do we measure that? One thought is to estimate accuracy on the test set. But wait! The test set should only be run AFTER we're done training everything or else our final results will be fudged. Here's an idea: let's use our knowledge of k-fold cross validation to split up our training set into a \"training set\" and a \"validation set\", and measure accuracy on the validation set. Average the accuracy over all k folds. What is the accuracy of your model?\n",
    "\n",
    "Perhaps [sklearn.model_selection.KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) would be useful.\n",
    "\n",
    "TODO say WOW the featurizations that have low training error and high test error are exactly the ones that have low variance, that was calculated in the previous part! O:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring the Bias and Variance of Your Model\n",
    "\n",
    "Is the model doing well?\n",
    "\n",
    "As noted in lecture, bias is generally expressed as a model's tendency to approximate certain functions even if conflicting features are in the training set, and variance is generally expressed as a model's difference in performance on the test set given a different training set. Also remember the irreducible error is that which cannot be eliminated because it is in our inherently noisy measurements of the labels.\n",
    "\n",
    "A mathematical formulation is below:\n",
    "\n",
    "$$\\text{Total Noise} = (E[h(x|D)] - f(x))^2 + Var(h(x|D)) + Var(Z)$$\n",
    "\n",
    "where h(x|D) is the model's prediction given a training dataset, f(x) is the true label, and Z is the inherent noise in the labels. These terms are bias, variance, and irreducible error, respectively. A detailed derivation can be found [here](https://www.eecs189.org/static/notes/n5.pdf) or in the notes.\n",
    "\n",
    "## The Game Plan\n",
    "\n",
    "1. Since these values are evaluated over many different training datasets, let's structure this like k-fold cross validation so we can randomly sample datasets. Perhaps, you can use [sklearn.model_selection.KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html). **NOTE:** X_test must be the same for all datasets for the bias variance measurement corresponding to the above to be correct.\n",
    "\n",
    "2. For each of the k splits, \n",
    " - train your model using your selected features\n",
    " - record predictions for each test datapoint x\n",
    "\n",
    "3. After gathering the above information, average the predicted label over the k splits for each input x to obtain E[h(x|D)] and combine with the appropriate y label f(x). Average these values over inputs x to get the bias\n",
    "\n",
    "4. Compute the variance of predictions for each input x. Then average over inputs x to get the variance Var(h(x|D))\n",
    "\n",
    "What is the bias of your model? The variance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this  function is COMPLETELY untested\n",
    "\n",
    "def get_bias_variance(X, y, feat_selector):\n",
    "    \"\"\"\n",
    "    X- the original training data\n",
    "    y- the labels for the original training data\n",
    "    feat_selector- a function created by create_function_selector()\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    ##### START STEP 1 #####\n",
    "    n_rest = int(X.shape[0] * 0.75)\n",
    "    X_rest, X_test = X[:n_rest], X[n_rest:]\n",
    "    y_rest, y_test = y[:n_rest], y[n_rest:]\n",
    "    \n",
    "    kf = sklearn.model_selection.KFold(n_splits=4)\n",
    "    for train_index, test_index in kf.split(X_rest):\n",
    "        X_train, y_train = X[train_index], y[train_index]\n",
    "    ##### END STEP 1 #####\n",
    "        ##### START STEP 2 #####\n",
    "        reg = train(X_train, y_train)\n",
    "        predictions.append(reg.predict(X_test))\n",
    "        true_labels.append(y_test)\n",
    "        ##### END STEP 2 #####\n",
    "    \n",
    "    ##### START STEP 3 #####\n",
    "    bias = np.mean((np.mean(predictions, axis=0) - true_labels[0])**2)\n",
    "    ##### END STEP 3 #####\n",
    "    \n",
    "    ##### START STEP 4 #####\n",
    "    variance = np.std(np.mean(predictions, axis=0))\n",
    "    ##### END STEP 4 #####\n",
    "    \n",
    "    return bias, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.4898979485566356)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bias_variance(X, y, create_feat_selector(['product name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO something about repeating the above until your model prediction accuracy is ~90% (base this percentage on the best staff solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
