{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS189 Project T Final Notebook\n",
    "## Week 2: Testing/Training, Cross-Validation, and Bias-Variance\n",
    "## Student Copy\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "# data organization libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization libraries\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# modeling libraries\n",
    "import sklearn as sk\n",
    "from sklearn import kernel_ridge\n",
    "\n",
    "plt.style.use('seaborn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "\n",
    "Prediction models that are used in the industry must be able to maintain accuracy on previously unseen data. At this point in your EECS education, you have only learned model assesment within the context of data the model has already seen. This creates a problem, because if we assess our model with the same data that was used to fit it, then we may overestimate how well our model does at prediction. After completing this assigment, students will know the methodology behind improving prediction models so they are ready for use in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "Suppose that you have been contracted by the Portuguese Government to conduct data driven research on forest fire prevention. The main goal of your research is to create a model that will be used by a government fast forrest fire detection system. Can you predict the area of a fire (and thus its severity) based on certain weather conditions?\n",
    "    \n",
    "The data set we are given consists of 517 seperate forrest fire instances from different areas within the Montesinho Natural Park in Portugal, with features being the weather and climate conditions recorded the day of the fire.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Fires Data\n",
    "\n",
    "![](park.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a map of the park that our data set comes from. Lets download a dataset and  take a look at the first couple of rows. We will be loading in a dataset with fewer columns than the one you will use later in this notebook to look at some key ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df = pd.read_csv('raw_df.csv')\n",
    "fire_df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X and Y represent the location of the fire on the simplied map provided above\n",
    "- month and day are for the date of the fire\n",
    "- area is the extent of the fire\n",
    "- wind and rain (and other measures we will introduce later) describe the environmental conditions at the time\n",
    "    \n",
    "The response variable, meaning the value we are trying to predict, can be found in the area column, which represents the total area burned per Hectares(ha). The rest of the columns are the weather and climate features we will be using to fit our model. Wind is wind speed in $km/h$ and Rain is outside rain in $mm/m^2$. The other columns represent Codes from the Canadian Fire Weather Index. Below is a diagram outline what each Code means.\n",
    "\n",
    "![](fwi.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot** univariate histograms for each of the features in the data frame. The .hist() method of pandas dataframes may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### START #####\n",
    "\n",
    "##### END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot** the following joint distributions. Do you see any possible relationships?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. X and Y location with area being the size of the dot at that point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### START #####\n",
    "\n",
    "##### END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. FFMC and Wind with ISI as the size of the dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### START #####\n",
    "\n",
    "##### END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Some joint distribution of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### START #####\n",
    "\n",
    "##### END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing data to Numpy\n",
    "\n",
    "Pandas is really good for filtering and manipulating large datasets, but when we finally have to train machine learning models, it's best to keep the data in a matrix format. This will make computations run faster and give us access to helpful libraries like sklearn.\n",
    "\n",
    "Use the df.[to_numpy()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html) function in pandas to create the feature and label matrices X and y. Since we're predicting fire area, what column represents a data point's label (what we want to predict)? What columns represent the data point's features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### START #####\n",
    "\n",
    "##### END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data & Test Data\n",
    "\n",
    "We only have so much data and we need to decide how much to set aside for testing and how much to use for training. We cannot use the testing data in training because we may overfit to particular details of the limited data set instead of generalizing to the underlying pattern that generated the data.\n",
    "\n",
    "It would be nice to use as much data as we can to train since we need lots of data to learn the overall pattern. While doing this, we should still have a hefty amount to be used to test since we need to know how well the model is performing.\n",
    "\n",
    "Let's say we want to do a 7:3 training-test split. Without using sklearn's `train_test_split` function, create matrices `X_train`, `y_train`, `X_test`, and `y_test` to reflect this split. This will help you to understand what this function is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### START #####\n",
    "\n",
    "##### END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we see how to split apart the data, we will just use sklearn's `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### START #####\n",
    "\n",
    "##### END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine the test train split and numpy conversion into a generic function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_test_train(df, target, features, test_ratio):\n",
    "    # df - data frame we are using to pull information\n",
    "    # target - the data frame column  to predict\n",
    "    # features - the columns for x\n",
    "    # test_ratio - was ratio of the data to dedicate towards testing\n",
    "    \n",
    "    ##### START #####\n",
    "    \n",
    "    ##### END #####\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sk_test_train(fire_df, 'area', ['temp', 'rain', 'wind'], 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares\n",
    "\n",
    "In this section we will begin to build the linear models we will be evaluating later. We will be using different variants of the OLS and SVM models you have been exposed to previously. Here is a diagram to refresh your knowledge of OLS. For move review material see this note from EE16A linked below.\n",
    "[OLS_REVIEW_16A](https://eecs16a.org/lecture/Note23.pdf)\n",
    "\n",
    "    \n",
    "![](OLS.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement!\n",
    "\n",
    "Now that you have a better understanding of the data and models we will be working with,  we will get some practice with the Scikit-Learn functions that will be used throughout this assigment. We will start with the LinearRegression class from Scikit-Learn's linear_model library. Perhaps use sklearn's\n",
    "[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(X_train, y_train):\n",
    "    \"\"\"\n",
    "    X_train - Training data\n",
    "    y_train - Training labels\n",
    "    \n",
    "    Return reg, an instance of LinearRegression.fit() that represents the trained model\n",
    "    \"\"\"\n",
    "    ##### START #####\n",
    "\n",
    "    ##### END #####\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing Metrics\n",
    "\n",
    "How close are the model predictions to the true labels? Let's implement some error metrics with sklearn to see.\n",
    "\n",
    "Implement \n",
    "- Mean Squared Error (MSE) without sklearn functions, and with sklearn functions like [mean_squared_error()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)\n",
    "- Root Mean Squared Error (RMSE) without sklearn functions, and with sklearn functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse_naive(y, y_hat):\n",
    "    \"\"\"\n",
    "    Calculate the MSE with numpy functions\n",
    "    Do not use any sklearn functions\n",
    "    \n",
    "    y - Labels for the data\n",
    "    y_hat - Predicted label for the data\n",
    "    \n",
    "    return MSE\n",
    "    \"\"\"\n",
    "    ##### START #####\n",
    "    \n",
    "    ##### END #####\n",
    "\n",
    "def get_rmse_naive(y, y_hat):\n",
    "    \"\"\"\n",
    "    Calculate the RMSE with numpy functions\n",
    "    Do not use any sklearn functions\n",
    "    \n",
    "    y - Labels for the data\n",
    "    y_hat - Predicted label for the data\n",
    "    \n",
    "    return RMSE\n",
    "    \"\"\"\n",
    "    ##### START #####\n",
    "    \n",
    "    ##### END #####\n",
    "\n",
    "def get_mse(y, y_hat):\n",
    "    \"\"\"\n",
    "    Calculate the MSE with sklearn functions\n",
    "    \n",
    "    y - Labels for the data\n",
    "    y_hat - Predicted label for the data\n",
    "    \n",
    "    Return MSE\n",
    "    \"\"\"\n",
    "    ##### START #####\n",
    "    \n",
    "    ##### END #####\n",
    "\n",
    "def get_rmse(y, y_hat):\n",
    "    \"\"\"\n",
    "    Calculate the RMSE with sklearn and numpy functions\n",
    "    \n",
    "    y - Labels for the data\n",
    "    y_hat - Predicted label for the data\n",
    "    \n",
    "    Return RMSE\n",
    "    \"\"\"\n",
    "    ##### START #####\n",
    "    \n",
    "    ##### END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementations with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, y_hat = np.array([1, 2, 3]), np.array([2, 0, 3])\n",
    "assert np.isclose(get_mse_naive(y, y_hat), 1.6666666666666667), \"Got {} but expected 1.6666666666666667\".format(get_mse_naive(y, y_hat))\n",
    "assert np.isclose(get_rmse_naive(y, y_hat), 1.2909944487358056), \"Got {} but expected 1.2909944487358056\".format(get_rmse_naive(y, y_hat))\n",
    "assert np.isclose(get_mse(y, y_hat), 1.6666666666666667), \"Got {} but expected 1.6666666666666667\".format(get_mse(y, y_hat))\n",
    "assert np.isclose(get_rmse(y, y_hat), 1.2909944487358056), \"Got {} but expected 1.2909944487358056\".format(get_rmse(y, y_hat))\n",
    "print(\"Passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models\n",
    "\n",
    "Now we want to take the training data we set aside and use it to train models that can correctly identify the underlying pattern. Do not focus too much on the models we use for now; consider it a preview for the rest of this course.\n",
    "\n",
    "We will be evaluating the training error for each model, but remember that if we focus too much on lowering training data we might run into overfitting.\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Ok let's put these functions to the test! Report the training error using MSE and RMSE with the LinearRegression model\n",
    "\n",
    "Try to get testing MSE below 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg_model = train_linear_regression(X_train, y_train)\n",
    "\n",
    "##### START How do you get predictions from the model? See sklearn examples for LinearRegression #####\n",
    "\n",
    "##### END #####\n",
    "\n",
    "print('Training MSE:', get_mse(y_train, y_hat)) #model might be underfitting\n",
    "print('Training RMSE:', get_rmse(y_train, y_hat)) #model might be underfitting\n",
    "\n",
    "##### START #####\n",
    "\n",
    "##### END #####\n",
    "print()\n",
    "print('Testing MSE:', get_mse(y_test, y_hat_test)) \n",
    "print('Testing RMSE:', get_rmse(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we do with other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR (the regression version of SVM)\n",
    "\n",
    "Last week, you looked at SVMs, another linear model. sklearn includes an implementation that explicitly does regression for us. It is [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html). \n",
    "\n",
    "Complete the training step, which takes into account the parameters C and gamma that are passed into SVR()\n",
    "\n",
    "Try different hyperparameter values, for C and gamma, to minimize the MSE and RMSE.\n",
    "\n",
    "Try to get testing MSE below 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SVR(X_train, y_train, gamma=0.2):\n",
    "    \"\"\"\n",
    "    X_train - Training data\n",
    "    y_train - Training labels\n",
    "    C - a hyperparameter for SVC, Regularization parameter\n",
    "    gamma - a hyperparameter for SVC, Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’\n",
    "    \n",
    "    Return reg, an instance of LinearRegression.fit() that represents the trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    ##### START #####\n",
    "\n",
    "    ##### END #####\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_model = train_SVR(X_train, y_train, gamma=1e3)\n",
    "# C -> Regularization parameter\n",
    "# gamma -> Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’\n",
    "# degree, default=3, Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.\n",
    "\n",
    "##### START How do you get predictions from the model? See sklearn examples for LinearRegression #####\n",
    "\n",
    "##### END #####\n",
    "\n",
    "print('Training MSE:', get_mse(y_train, y_hat)) #model might be underfitting\n",
    "print('Training RMSE:', get_rmse(y_train, y_hat)) #model might be underfitting\n",
    "\n",
    "##### START #####\n",
    "\n",
    "##### END #####\n",
    "print()\n",
    "print('Testing MSE:', get_mse(y_test, y_hat_test)) \n",
    "print('Testing RMSE:', get_rmse(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge Regression (rbf kernel)\n",
    "Hm, it seems like svr doesn't do much better than simple linear regression\n",
    "Let's try adjusting parameters for another model and see where that gets us.\n",
    "\n",
    "Your goal is to have testing MSE below 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_KernelRidge(X_train, y_train, gamma=0.2):\n",
    "    \"\"\"\n",
    "    X_train - Training data\n",
    "    y_train - Training labels\n",
    "    gamma - a hyperparameter for SVC, Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’\n",
    "    \n",
    "    Return reg, an instance of LinearRegression.fit() that represents the trained model\n",
    "    \"\"\"\n",
    "    ##### START #####\n",
    "\n",
    "    ##### END #####\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernelRidge_model = train_KernelRidge(X_train, y_train, gamma=1e2)\n",
    "# gamma -> Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’\n",
    "\n",
    "##### START How do you get predictions from the model? See sklearn examples for LinearRegression #####\n",
    "\n",
    "##### END #####\n",
    "\n",
    "print('Training MSE:', get_mse(y_train, y_hat)) #model might be underfitting\n",
    "print('Training RMSE:', get_rmse(y_train, y_hat)) #model might be underfitting\n",
    "\n",
    "##### START #####\n",
    "\n",
    "##### END #####\n",
    "print()\n",
    "print('Testing MSE:', get_mse(y_test, y_hat_test)) \n",
    "print('Testing RMSE:', get_rmse(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the model\n",
    "\n",
    "Congrats! You have created base models for this fire area prediction problem. Now it is time to tune them to do better on the test set. \n",
    "\n",
    "### How do you actually make this model do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation\n",
    "\n",
    "An alternative to splitting out data into testing and training sections is k-fold cross validation. This will allow us to use all the data for testing and all the data for testing without mixing the sets at any one time. The only drawback is that we will have to do k times as much computation.\n",
    "\n",
    "How this works is we split the data into k equal sections, pick one of those sections for validation and the rest of the sections for training. We repeat this k times, each time picking a different section for the validation. After doing this k times, we average the errors and that is our estimation of true error.\n",
    "\n",
    "Since more data for training and testing helps us, this method should yield more accurate result with the only drawback being that we have to do k times as much computation to get our results.\n",
    "\n",
    "Now implement k-fold cross validation find a more accurate estimation of the true error of the models we have provided.\n",
    "\n",
    "Perhaps [sklearn.model_selection.KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) would be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse_with_k_fold(train_fn, gamma=1e0, gammaPresent=False):\n",
    "    ##### START #####\n",
    "\n",
    "    ##### END #####\n",
    "    return mse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear: Average MSE with K-Fold Cross Validation:\", get_mse_with_k_fold(train_linear_regression))\n",
    "print(\"SVR: Average MSE with K-Fold Cross Validation:\", get_mse_with_k_fold(train_SVR, gamma=1e3, gammaPresent=True))\n",
    "print(\"Kernel Ridge: Average MSE with K-Fold Cross Validation:\", get_mse_with_k_fold(train_KernelRidge, gamma=1e2, gammaPresent=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next section will show how the features with low training error correspond to the high test error.\n",
    "Low training error with high test error indicates that there is overfitting which usually is related to having too much variance in our features. This means the features are fitting noise in the data instead of the underlying pattern that generated the data.\n",
    "\n",
    "# Measuring the Bias and Variance of Your Model\n",
    "\n",
    "Is the model doing well?\n",
    "\n",
    "As noted in lecture, bias is generally expressed as a model's tendency to approximate certain functions even if conflicting features are in the training set, and variance is generally expressed as a model's difference in performance on the test set given a different training set. Also remember the irreducible error is that which cannot be eliminated because it is in our inherently noisy measurements of the labels.\n",
    "\n",
    "A mathematical formulation is below:\n",
    "\n",
    "$$\\text{Total Noise} = \\underbrace{(E[h(x|D)] - f(x))^2}_\\text{Bias} + \\underbrace{Var(h(x|D))}_\\text{Variance} + \\underbrace{Var(Z)}_\\text{Irreducible Noise}$$\n",
    "\n",
    "where *h(x|D)* is the model's prediction given a training dataset, *f(x)* is the true label, and *Z* is the inherent noise in the labels. These terms are bias, variance, and irreducible error, respectively. A detailed derivation can be found [here](https://www.eecs189.org/static/notes/n5.pdf) or in the notes.\n",
    "\n",
    "## The Game Plan - Calculating Bias and Variance\n",
    "\n",
    "1. Since these values are evaluated over many different training datasets, let's structure this like k-fold cross validation so we can randomly sample datasets. Perhaps, you can use [sklearn.model_selection.KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html).\n",
    "**NOTE:** `X_test` must be the same for all datasets for the bias variance measurement corresponding to the above to be correct.\n",
    "\n",
    "2. For each of the `k` splits, \n",
    " - train your model using your selected features\n",
    " - record predictions for each test datapoint `x`\n",
    "\n",
    "3. After gathering the above information, average the predicted label over the `k` splits for each input `x` to obtain *E[h(x|D)]* and combine with the appropriate `y` label *f(x)*. Average these values over inputs `x` to get the bias\n",
    "\n",
    "4. Compute the variance of predictions for each input `x`. Then average over inputs `x` to get the variance *Var(h(x|D))*\n",
    "\n",
    "### What is the bias of your model? The variance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this  function is COMPLETELY untested\n",
    "\n",
    "def get_bias_variance(X, y, train_fn, gamma=1e3, gammaPresent=False):\n",
    "    \"\"\"\n",
    "    X- the original training data\n",
    "    y- the labels for the original training data\n",
    "    train_fn- the function to train a model\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    ##### START STEP 1 #####\n",
    "    \n",
    "    ##### END STEP 1 #####\n",
    "        ##### START STEP 2 #####\n",
    "        \n",
    "        ##### END STEP 2 #####\n",
    "    \n",
    "    ##### START STEP 3 #####\n",
    "    \n",
    "    ##### END STEP 3 #####\n",
    "    \n",
    "    ##### START STEP 4 #####\n",
    "    \n",
    "    ##### END STEP 4 #####\n",
    "    \n",
    "    return bias, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearBias, linearVariance = get_bias_variance(X_train, y_train, train_linear_regression)\n",
    "print(\"Linear Bias:\", linearBias)\n",
    "print(\"Linear Variance:\", linearVariance)\n",
    "print()\n",
    "\n",
    "svrBias, svrVariance = get_bias_variance(X_train, y_train, train_SVR, 1e0, True)\n",
    "print(\"SVR Bias:\", svrBias)\n",
    "print(\"SVR Variance:\", svrVariance)\n",
    "print()\n",
    "\n",
    "kernelRidgeBias, kernelRidgeVariance = get_bias_variance(X_train, y_train, train_KernelRidge, 1e0, True)\n",
    "print(\"Kernel Ridge Bias:\", kernelRidgeBias)\n",
    "print(\"Kernel Ridge Variance:\", kernelRidgeVariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Selection!\n",
    "\n",
    "Once you meet all error requirements above, which model would you submit to the Portugese government? Report your accuracy on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When the dataset is biased\n",
    "\n",
    "Not only does the choice of model affect the accuracy of your system, but also the distribution of the dataset when you compare the training and test sets. Let's look at this biased dataset below. For example, plot the distributions of temperature with histograms for both the training and test data. What do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "biased_train_df = pd.read_csv('train_set_v1.csv')\n",
    "biased_test_df = pd.read_csv('test_set_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### START plot training distribution for temperature #####\n",
    "\n",
    "##### END #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### START plot test distribution for temperature #####\n",
    "\n",
    "##### END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is quite the distribution shift! Can we make these two distributions more similar so that the training stage will better prepare the model to predict for real-world test data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is provided for you. How do you resample the data with these functions?\n",
    "\n",
    "def filter_df(df, names, keys):\n",
    "    \"\"\"\n",
    "    Only keep rows whose column values match the keys\n",
    "    df - dataframe\n",
    "    names - column names to look at\n",
    "    keys - values that should be in each column\n",
    "    \"\"\"\n",
    "    if len(keys) == 2:\n",
    "        ret = df[(df[names[0]] == keys[0]) & (df[names[1]] == keys[1])]\n",
    "    else:\n",
    "        ret = df[df[names[0]] == keys[0]]\n",
    "    return ret\n",
    "\n",
    "def sample_dic(cat_dic, size):\n",
    "    \"\"\"\n",
    "    Scales proprotions to number of samples\n",
    "    \n",
    "    cat_dic - category dictionary that maps column index to its representation in the new dataset (between 0 and 1)\n",
    "    size - size of the dataset\n",
    "    \"\"\"\n",
    "    samp_dic = {}\n",
    "    for i in cat_dic:\n",
    "        samp_dic[i] = int(size * cat_dic[i])\n",
    "    return samp_dic\n",
    "\n",
    "def gen_sample(df, num, rand):\n",
    "    \"\"\"\n",
    "    Samples from a dataframe\n",
    "    df - the dataframe\n",
    "    num - number of samples\n",
    "    rand - random seed\n",
    "    \"\"\"\n",
    "    return df.sample(n=num, random_state=rand, replace=True)\n",
    "\n",
    "def biased_sample(df, size, bias_dics):\n",
    "    \"\"\"\n",
    "    Generates a new dataframe that keeps different proportions of the original data, grouped by value\n",
    "    \n",
    "    df - dataframe\n",
    "    size - num samples in resulting dataframe\n",
    "    bias_dics - proportion of the dataset that should have those values\n",
    "    \"\"\"\n",
    "    ret_df = pd.DataFrame()\n",
    "    samp_dic = sample_dic(bias_dics[0], size)\n",
    "    \n",
    "    first = True\n",
    "    for i in samp_dic:\n",
    "        names = ['temp_bins']\n",
    "        keys = [i]\n",
    "        data = filter_df(df, names, keys)\n",
    "        rs = int(1)\n",
    "        b_samp = gen_sample(data, int(samp_dic[i]), rs)\n",
    "        if first == True:\n",
    "            ret_df = b_samp\n",
    "        else:\n",
    "            ret_df = pd.concat([ret_df, b_samp])\n",
    "        first = False\n",
    "    return ret_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### START #####\n",
    "\n",
    "##### END #####\n",
    "un_biased.hist(column='temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: Methods such as these can help identify what to bootstrap, but are not necessarily the methodologies used in practice.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
