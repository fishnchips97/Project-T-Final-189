# EECS189 Project T Final
## Team Sean Project T Final
# Week 2: Bias variance, Cross Validation, Training/Testing
### Team Sean: Sean Betancourt, Erik Fisher, and Jenny Wang


## Goals:
Prediction models that are used in the industry must be able to maintain accuracy on previously unseen data. At this point in your EECS education, you have only learned model assessment within the context of data the model has already seen. This creates a problem, because if we assess our model with the same data that was used to fit it, then we may overestimate how well our model does at prediction. After completing this assignment, students will know the methodology behind improving prediction models so they are ready for use in the real world.

In sum, we hope students will:
- Learn the general process for training and testing machine learning models
- Determine how and why the model needs to fit the dataset
- Learn how to properly divide data between testing and training
- Use cross validation to tune parameters that affect the overall model (“hyperparameters” to be learned later) as seen in models like SVM
- Learn about sources of error and how it can come from irreducible noise, variance or the bias of the model

## How to navigate repo:
- Look in the “final” folder for the final submission
- Open the jupyter notebook provided to the student and install appropriate libraries as needed using `pip install`.
- The empty quiz pdf, quiz solution, slides, and notes are found in the “Companion Documents” folder
